# Sonic_Training

# ğŸ“– æ¦‚è¿°
Sonicæ˜¯ä¸€ä¸ªåŸºäºéŸ³é¢‘é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥å°†é™æ€å›¾åƒè½¬æ¢ä¸ºä¸éŸ³é¢‘åŒæ­¥çš„åŠ¨æ€è§†é¢‘ã€‚åŸä½œè€…åªå¼€æºäº†æ¨ç†ä»£ç ï¼Œæœ¬é¡¹ç›®ä¸»è¦ä¸ºäº†å¤ç°å…¶è®­ç»ƒä»£ç  â€œSonic: Shifting Focus to Global Audio Perception in Portrait Animation, CVPR 2025â€ åŸä½œè€…æ‰€å±å•ä½ä¸ºè…¾è®¯å’Œæµ™æ±Ÿå¤§å­¦


## Sonic: Shifting Focus to Global Audio Perception in Portrait Animation

åŸºäºCVPR 2025è®ºæ–‡çš„éŸ³é¢‘é©±åŠ¨è‚–åƒåŠ¨ç”»å®ç°ã€‚

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.7+ (æ¨èä½¿ç”¨GPU)
- è‡³å°‘32GB GPUå†…å­˜ï¼ˆæ¨èï¼‰

### å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n sonic python=3.8
conda activate sonic

# å®‰è£…PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…å…¶ä»–ä¾èµ–
pip install diffusers transformers accelerate
pip install opencv-python moviepy librosa soundfile
pip install gradio wandb tqdm einops
pip install face_recognition
```

### ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹

1. **ä¸‹è½½Sonicæ¨¡å‹æƒé‡**ï¼š
```bash
# ä½¿ç”¨huggingface-cli
pip install huggingface_hub[cli]
huggingface-cli download LeonJoe13/Sonic --local-dir checkpoints
```

2. **ä¸‹è½½ä¾èµ–æ¨¡å‹**ï¼š
```bash
# SVDæ¨¡å‹
huggingface-cli download stabilityai/stable-video-diffusion-img2vid-xt --local-dir checkpoints/stable-video-diffusion-img2vid-xt

# Whisperæ¨¡å‹
huggingface-cli download openai/whisper-tiny --local-dir checkpoints/whisper-tiny
```

## ğŸ’» ä½¿ç”¨æ–¹æ³•

### 1. æ¨ç†æ¼”ç¤º

#### å‘½ä»¤è¡Œæ–¹å¼
```bash
python sonic_demo.py \
    --checkpoint checkpoints/Sonic \
    --image examples/portrait.jpg \
    --audio examples/audio.wav \
    --output output.mp4 \
    --motion moderate \
    --seed 42
```

#### Gradioç•Œé¢
```bash
python sonic_demo.py --checkpoint checkpoints/Sonic --gradio --share
```

### 2. è®­ç»ƒæ¨¡å‹

å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆMP4è§†é¢‘æ–‡ä»¶ï¼‰ï¼Œç„¶åè¿è¡Œï¼š

```bash
python sonic_training.py \
    --model_path checkpoints/stable-video-diffusion-img2vid-xt \
    --data_root /path/to/training/videos \
    --output_dir /path/to/save/checkpoints \
    --train_batch_size 1 \
    --num_train_epochs 100 \
    --learning_rate 1e-5 \
    --mixed_precision fp16 \
    --num_frames 25 \
    --resolution 576 \
    --use_wandb
```

### 3. åœ¨ä»£ç ä¸­ä½¿ç”¨

```python
from sonic_implementation import SonicConfig, SonicPipeline
import cv2

# åˆå§‹åŒ–
config = SonicConfig()
pipeline = SonicPipeline(config)

# åŠ è½½å›¾åƒå’ŒéŸ³é¢‘
reference_image = cv2.imread("portrait.jpg")
audio_features = extract_audio_features("audio.wav")

# ç”Ÿæˆè§†é¢‘
video = pipeline.generate(
    reference_image=reference_image,
    audio_features=audio_features,
    num_frames=25,
    motion_buckets={'translation': 64, 'expression': 96}
)

# ä¿å­˜è§†é¢‘
save_video(video, "output.mp4")
```

## ğŸ“Š è®­ç»ƒç»†èŠ‚

### æ•°æ®å‡†å¤‡

è®­ç»ƒæ•°æ®åº”è¯¥æ˜¯åŒ…å«æ¸…æ™°éŸ³é¢‘å’Œé¢éƒ¨ç”»é¢çš„MP4è§†é¢‘æ–‡ä»¶ï¼š
- å»ºè®®è§†é¢‘é•¿åº¦ï¼š5-30ç§’
- åˆ†è¾¨ç‡ï¼šè‡³å°‘512x512
- éŸ³é¢‘ï¼šæ¸…æ™°çš„è¯´è¯å£°
- æœ¬é¡¹ç›®ä¸­æˆ‘ä»¬ä¸»è¦é‡‡ç”¨äº†CelebV-HQçš„æ•°æ®è¿›è¡Œäº†è®­ç»ƒã€‚å¯¹äºç›¸åº”çš„æ•°æ®é›†ä¸‹è½½å’Œå¤„ç†è¯·å‚ç…§CelebV-HQ (https://github.com/CelebV-HQ/CelebV-HQ) çš„åŸå§‹æ–¹æ¡ˆã€‚


ç›®å½•ç»“æ„ï¼š
```
data_root/
â”œâ”€â”€ video1.mp4
â”œâ”€â”€ video2.mp4
â”œâ”€â”€ ...
â””â”€â”€ videoN.mp4
```

### è®­ç»ƒå‚æ•°è¯´æ˜

- `train_batch_size`: æ‰¹æ¬¡å¤§å°ï¼ˆå»ºè®®1ï¼Œå› ä¸ºè§†é¢‘å ç”¨å†…å­˜å¤§ï¼‰
- `gradient_accumulation_steps`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆç”¨äºæ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡ï¼‰
- `num_frames`: æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„å¸§æ•°ï¼ˆé»˜è®¤25ï¼‰
- `resolution`: è®­ç»ƒåˆ†è¾¨ç‡ï¼ˆé»˜è®¤576ï¼‰
- `mixed_precision`: æ··åˆç²¾åº¦è®­ç»ƒï¼ˆfp16æ¨èï¼‰

### è¿åŠ¨æ§åˆ¶å‚æ•°

- **Translation Bucket** (0-127): æ§åˆ¶å¤´éƒ¨è¿åŠ¨å¹…åº¦
  - 0-32: è½»å¾®è¿åŠ¨
  - 32-96: ä¸­ç­‰è¿åŠ¨
  - 96-127: å‰§çƒˆè¿åŠ¨

- **Expression Bucket** (0-127): æ§åˆ¶è¡¨æƒ…å˜åŒ–å¼ºåº¦
  - 0-32: ç»†å¾®è¡¨æƒ…
  - 32-96: æ­£å¸¸è¡¨æƒ…
  - 96-127: å¤¸å¼ è¡¨æƒ…

- **Dynamic Scale**: æ•´ä½“è¿åŠ¨å¼ºåº¦
  - 1.0: æ¸©å’Œï¼ˆmildï¼‰
  - 1.5: é€‚ä¸­ï¼ˆmoderateï¼‰
  - 2.0: å¼ºçƒˆï¼ˆintenseï¼‰

## ğŸ”§ é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰éŸ³é¢‘ç‰¹å¾æå–

```python
def custom_audio_features(audio_path):
    # ä½¿ç”¨è‡ªå®šä¹‰çš„éŸ³é¢‘å¤„ç†
    audio, sr = librosa.load(audio_path, sr=16000)
    
    # æå–MFCCã€éŸ³é«˜ç­‰ç‰¹å¾
    mfcc = librosa.feature.mfcc(y=audio, sr=sr)
    pitch = librosa.piptrack(y=audio, sr=sr)
    
    # ç»„åˆç‰¹å¾
    features = combine_features(mfcc, pitch)
    return features
```

### æ‰¹é‡å¤„ç†

```python
def batch_process(image_paths, audio_paths, output_dir):
    for img_path, audio_path in zip(image_paths, audio_paths):
        output_path = f"{output_dir}/{Path(img_path).stem}.mp4"
        pipeline.generate_video(
            image_path=img_path,
            audio_path=audio_path,
            output_path=output_path
        )
```

## ğŸ“ å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº†æœ¬ä»£ç ï¼Œè¯·å¼•ç”¨åŸå§‹è®ºæ–‡ï¼š

```bibtex
@article{ji2024sonic,
    title={Sonic: Shifting Focus to Global Audio Perception in Portrait Animation},
    author={Ji, Xiaozhong and Hu, Xiaobin and Xu, Zhihong and Zhu, Junwei and Lin, Chuming and He, Qingdong and Zhang, Jiangning and Luo, Donghao and Chen, Yi and Lin, Qin and others},
    journal={arXiv preprint arXiv:2411.16331},
    year={2024}
}
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨éå•†ä¸šè®¸å¯è¯ã€‚å¦‚éœ€å•†ä¸šä½¿ç”¨ï¼Œè¯·è”ç³»åŸä½œè€…ã€‚

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢Stable Video Diffusionå›¢é˜Ÿæä¾›çš„åŸºç¡€æ¨¡å‹
- æ„Ÿè°¢OpenAIæä¾›çš„Whisperæ¨¡å‹
- æ„Ÿè°¢æ‰€æœ‰å¼€æºç¤¾åŒºçš„è´¡çŒ®è€…

## å¸¸è§é—®é¢˜

### Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
- å‡å°`resolution`åˆ°512æˆ–æ›´å°
- å‡å°‘`num_frames`
- ä½¿ç”¨`gradient_accumulation_steps`
- å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ`--mixed_precision fp16`

### Q: ç”Ÿæˆçš„è§†é¢‘ä¸å¤Ÿæµç•…ï¼Ÿ
A: å¯ä»¥å°è¯•ï¼š
- å¢åŠ `shift_offset`å‚æ•°ï¼ˆé»˜è®¤3ï¼‰
- è°ƒæ•´è¿åŠ¨å¼ºåº¦å‚æ•°
- ä½¿ç”¨æ›´é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®

### Q: å¦‚ä½•æé«˜å”‡åŒæ­¥è´¨é‡ï¼Ÿ
A: 
- ç¡®ä¿éŸ³é¢‘è´¨é‡æ¸…æ™°
- è°ƒæ•´`guidance_scale_audio`å‚æ•°
- ä½¿ç”¨æ›´å¤šåŒ…å«æ¸…æ™°å”‡åŠ¨çš„è®­ç»ƒæ•°æ®
