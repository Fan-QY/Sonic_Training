"""
Sonicæ¨ç†æ¼”ç¤ºè„šæœ¬
ä½¿ç”¨è®­ç»ƒå¥½çš„Sonicæ¨¡å‹ç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„è‚–åƒåŠ¨ç”»
"""

import os
import torch
import numpy as np
import cv2
import argparse
from pathlib import Path
import librosa
import soundfile as sf
from moviepy.editor import VideoFileClip, AudioFileClip
from transformers import WhisperProcessor, WhisperModel
from diffusers import AutoencoderKL, DDIMScheduler
from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor
import gradio as gr
from typing import Optional, Tuple
import tempfile
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥Sonicç»„ä»¶
from sonic_implementation import SonicUNet, SonicConfig, SonicPipeline

class SonicDemo:
    """Sonicæ¼”ç¤ºç±»"""
    def __init__(self, checkpoint_path: str):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")
        
        # åŠ è½½æ¨¡å‹
        self._load_models(checkpoint_path)
        
    def _load_models(self, checkpoint_path: str):
        """åŠ è½½æ‰€æœ‰å¿…éœ€çš„æ¨¡å‹"""
        print("Loading models...")
        
        # åˆå§‹åŒ–é…ç½®
        self.config = SonicConfig()
        
        # åŠ è½½UNet
        self.unet = SonicUNet(self.config).to(self.device)
        unet_path = os.path.join(checkpoint_path, "unet.pth")
        if os.path.exists(unet_path):
            state_dict = torch.load(unet_path, map_location=self.device)
            self.unet.load_state_dict(state_dict)
            print(f"Loaded UNet from {unet_path}")
        else:
            print("Warning: UNet checkpoint not found, using random weights")
        
        self.unet.eval()
        
        # åŠ è½½VAE
        self.vae = AutoencoderKL.from_pretrained(
            "stabilityai/stable-video-diffusion-img2vid-xt",
            subfolder="vae"
        ).to(self.device)
        self.vae.eval()
        
        # åŠ è½½CLIP
        self.clip_processor = CLIPImageProcessor.from_pretrained(
            "stabilityai/stable-video-diffusion-img2vid-xt",
            subfolder="feature_extractor"
        )
        self.clip_model = CLIPVisionModelWithProjection.from_pretrained(
            "stabilityai/stable-video-diffusion-img2vid-xt",
            subfolder="image_encoder"
        ).to(self.device)
        self.clip_model.eval()
        
        # åŠ è½½Whisper
        self.whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-tiny")
        self.whisper_model = WhisperModel.from_pretrained("openai/whisper-tiny").to(self.device)
        self.whisper_model.eval()
        
        # åŠ è½½è°ƒåº¦å™¨
        self.scheduler = DDIMScheduler.from_pretrained(
            "stabilityai/stable-video-diffusion-img2vid-xt",
            subfolder="scheduler"
        )
        
        print("All models loaded successfully!")
    
    @torch.no_grad()
    def extract_audio_features(self, audio_path: str) -> torch.Tensor:
        """æå–éŸ³é¢‘ç‰¹å¾"""
        # åŠ è½½éŸ³é¢‘
        audio, sr = librosa.load(audio_path, sr=16000)
        
        # ä½¿ç”¨Whisperå¤„ç†
        inputs = self.whisper_processor(
            audio, 
            return_tensors="pt", 
            sampling_rate=16000
        ).to(self.device)
        
        # æå–ç‰¹å¾
        outputs = self.whisper_model(**inputs)
        
        # è·å–å¤šå±‚ç‰¹å¾
        features = []
        for hidden_state in outputs.encoder_hidden_states[-5:]:  # æœ€å5å±‚
            features.append(hidden_state)
        
        # æ‹¼æ¥ç‰¹å¾
        audio_features = torch.cat(features, dim=-1)
        audio_features = audio_features.squeeze(0)  # [T, D]
        
        return audio_features
    
    @torch.no_grad()
    def generate_video(self,
                      image_path: str,
                      audio_path: str,
                      output_path: str,
                      num_frames: int = 25,
                      fps: int = 25,
                      motion_intensity: str = "moderate",
                      seed: int = 42) -> str:
        """ç”Ÿæˆè§†é¢‘"""
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(seed)
        np.random.seed(seed)
        
        # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, (self.config.resolution, self.config.resolution))
        
        # å›¾åƒå½’ä¸€åŒ–
        image_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 127.5 - 1.0
        image_tensor = image_tensor.unsqueeze(0).to(self.device)
        
        # æå–CLIPç‰¹å¾
        clip_input = self.clip_processor(images=image, return_tensors="pt")
        clip_features = self.clip_model(clip_input['pixel_values'].to(self.device)).image_embeds
        
        # ç¼–ç å‚è€ƒå›¾åƒ
        reference_latent = self.vae.encode(image_tensor).latent_dist.sample()
        reference_latent = reference_latent * 0.18215
        
        # æå–éŸ³é¢‘ç‰¹å¾
        print("Extracting audio features...")
        audio_features = self.extract_audio_features(audio_path)
        
        # è®¾ç½®è¿åŠ¨å¼ºåº¦
        motion_scales = {
            "mild": 1.0,
            "moderate": 1.5,
            "intense": 2.0
        }
        self.config.dynamic_scale = motion_scales.get(motion_intensity, 1.5)
        
        # å‡†å¤‡æ½œåœ¨è¡¨ç¤º
        latent_shape = (1, 4, num_frames, 
                       self.config.resolution // 8, 
                       self.config.resolution // 8)
        latents = torch.randn(latent_shape, device=self.device)
        
        # è®¾ç½®è°ƒåº¦å™¨
        self.scheduler.set_timesteps(self.config.num_inference_steps)
        
        # æ‰©æ•£å»å™ªå¾ªç¯
        print("Generating video...")
        for i, t in enumerate(self.scheduler.timesteps):
            # å‡†å¤‡æ¨¡å‹è¾“å…¥
            latent_model_input = torch.cat([latents, reference_latent.unsqueeze(2).expand(-1, -1, num_frames, -1, -1)], dim=1)
            
            # æ— æ¡ä»¶å¼•å¯¼
            if self.config.guidance_scale_audio > 1.0:
                latent_model_input = torch.cat([latent_model_input] * 2)
                audio_features_input = torch.cat([
                    audio_features.unsqueeze(0),
                    torch.zeros_like(audio_features).unsqueeze(0)
                ])
                clip_features_input = torch.cat([clip_features, torch.zeros_like(clip_features)])
            else:
                audio_features_input = audio_features.unsqueeze(0)
                clip_features_input = clip_features
            
            # é¢„æµ‹å™ªå£°
            noise_pred = self.unet(
                latent_model_input,
                t.unsqueeze(0).to(self.device),
                encoder_hidden_states={'reference': reference_latent},
                audio_features=audio_features_input,
                clip_features=clip_features_input
            )
            
            # åº”ç”¨å¼•å¯¼
            if self.config.guidance_scale_audio > 1.0:
                noise_pred_cond, noise_pred_uncond = noise_pred.chunk(2)
                noise_pred = noise_pred_uncond + self.config.guidance_scale_audio * (
                    noise_pred_cond - noise_pred_uncond
                )
            
            # è°ƒåº¦å™¨æ­¥è¿›
            latents = self.scheduler.step(noise_pred, t, latents).prev_sample
            
            # æ˜¾ç¤ºè¿›åº¦
            if i % 5 == 0:
                print(f"Progress: {i+1}/{len(self.scheduler.timesteps)}")
        
        # è§£ç è§†é¢‘
        print("Decoding video...")
        latents = latents / 0.18215
        video_tensor = self.vae.decode(latents).sample
        
        # åå¤„ç†
        video_tensor = (video_tensor + 1) / 2
        video_tensor = torch.clamp(video_tensor, 0, 1)
        video_tensor = video_tensor.squeeze(0).permute(1, 2, 3, 0)  # [T, H, W, C]
        video_array = (video_tensor.cpu().numpy() * 255).astype(np.uint8)
        
        # ä¿å­˜è§†é¢‘
        print("Saving video...")
        temp_video_path = output_path.replace('.mp4', '_temp.mp4')
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(temp_video_path, fourcc, fps, 
                             (self.config.resolution, self.config.resolution))
        
        for frame in video_array:
            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            out.write(frame_bgr)
        
        out.release()
        
        # æ·»åŠ éŸ³é¢‘
        print("Adding audio...")
        video_clip = VideoFileClip(temp_video_path)
        audio_clip = AudioFileClip(audio_path)
        
        # è°ƒæ•´éŸ³é¢‘é•¿åº¦ä»¥åŒ¹é…è§†é¢‘
        video_duration = video_clip.duration
        if audio_clip.duration > video_duration:
            audio_clip = audio_clip.subclip(0, video_duration)
        
        # åˆæˆæœ€ç»ˆè§†é¢‘
        final_video = video_clip.set_audio(audio_clip)
        final_video.write_videofile(output_path, codec='libx264', audio_codec='aac')
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_video_path)
        video_clip.close()
        audio_clip.close()
        
        print(f"Video saved to {output_path}")
        return output_path

def create_gradio_interface(demo: SonicDemo):
    """åˆ›å»ºGradioç•Œé¢"""
    def process_video(image, audio, motion_intensity, seed):
        """å¤„ç†è§†é¢‘ç”Ÿæˆè¯·æ±‚"""
        try:
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp_img:
                image.save(tmp_img.name)
                image_path = tmp_img.name
            
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_audio:
                # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                sf.write(tmp_audio.name, audio[1], audio[0])
                audio_path = tmp_audio.name
            
            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            output_path = tempfile.mktemp(suffix='.mp4')
            
            # ç”Ÿæˆè§†é¢‘
            result_path = demo.generate_video(
                image_path=image_path,
                audio_path=audio_path,
                output_path=output_path,
                motion_intensity=motion_intensity,
                seed=seed
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(image_path)
            os.remove(audio_path)
            
            return result_path
            
        except Exception as e:
            return f"Error: {str(e)}"
    
    # åˆ›å»ºç•Œé¢
    with gr.Blocks(title="Sonic - éŸ³é¢‘é©±åŠ¨è‚–åƒåŠ¨ç”»") as interface:
        gr.Markdown("""
        # ğŸµ Sonic - éŸ³é¢‘é©±åŠ¨è‚–åƒåŠ¨ç”»
        
        ä¸Šä¼ ä¸€å¼ è‚–åƒå›¾ç‰‡å’Œä¸€æ®µéŸ³é¢‘ï¼Œç”Ÿæˆä¸éŸ³é¢‘åŒæ­¥çš„è¯´è¯è§†é¢‘ã€‚
        
        **æç¤ºï¼š**
        - å›¾ç‰‡æœ€å¥½æ˜¯æ­£é¢è‚–åƒï¼ŒèƒŒæ™¯ç®€å•
        - éŸ³é¢‘æœ€å¥½æ˜¯æ¸…æ™°çš„è¯´è¯å£°
        - ç”Ÿæˆè¿‡ç¨‹å¯èƒ½éœ€è¦1-2åˆ†é’Ÿ
        """)
        
        with gr.Row():
            with gr.Column():
                image_input = gr.Image(label="è‚–åƒå›¾ç‰‡", type="pil")
                audio_input = gr.Audio(label="éŸ³é¢‘æ–‡ä»¶", type="numpy")
                
                with gr.Row():
                    motion_intensity = gr.Radio(
                        choices=["mild", "moderate", "intense"],
                        value="moderate",
                        label="è¿åŠ¨å¼ºåº¦"
                    )
                    seed = gr.Slider(
                        minimum=0,
                        maximum=999999,
                        value=42,
                        step=1,
                        label="éšæœºç§å­"
                    )
                
                generate_btn = gr.Button("ç”Ÿæˆè§†é¢‘", variant="primary")
                
            with gr.Column():
                video_output = gr.Video(label="ç”Ÿæˆç»“æœ")
        
        # ç¤ºä¾‹
        gr.Examples(
            examples=[
                ["examples/portrait1.jpg", "examples/audio1.wav", "moderate", 42],
                ["examples/portrait2.jpg", "examples/audio2.wav", "intense", 123],
            ],
            inputs=[image_input, audio_input, motion_intensity, seed],
            outputs=video_output,
            fn=process_video,
            cache_examples=True
        )
        
        # ç»‘å®šäº‹ä»¶
        generate_btn.click(
            fn=process_video,
            inputs=[image_input, audio_input, motion_intensity, seed],
            outputs=video_output
        )
    
    return interface

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Sonic Demo")
    parser.add_argument("--checkpoint", type=str, required=True, help="Path to model checkpoint")
    parser.add_argument("--image", type=str, help="Path to input image")
    parser.add_argument("--audio", type=str, help="Path to input audio")
    parser.add_argument("--output", type=str, default="output.mp4", help="Output video path")
    parser.add_argument("--motion", type=str, default="moderate", 
                       choices=["mild", "moderate", "intense"], help="Motion intensity")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--gradio", action="store_true", help="Launch Gradio interface")
    parser.add_argument("--share", action="store_true", help="Share Gradio interface")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¼”ç¤º
    demo = SonicDemo(args.checkpoint)
    
    if args.gradio:
        # å¯åŠ¨Gradioç•Œé¢
        interface = create_gradio_interface(demo)
        interface.launch(share=args.share)
    else:
        # å‘½ä»¤è¡Œæ¨¡å¼
        if not args.image or not args.audio:
            print("Error: Please provide both --image and --audio paths")
            return
        
        demo.generate_video(
            image_path=args.image,
            audio_path=args.audio,
            output_path=args.output,
            motion_intensity=args.motion,
            seed=args.seed
        )

if __name__ == "__main__":
    main()
